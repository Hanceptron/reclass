♪♪ Come on, let's go. Good luck. Thank you very much. Good luck. Brother! Brother! Brother! I'm here, what's going on? I'm waiting, he's waiting. Calm down, I'm coming down. What are you doing? Is this a door? Yes. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here. I'm here.

Wikipedia, we can go with five, right? We can change this to be whatever we want. Then we can say the doc content characters max. I'll just make this equal to 100 because this is a quick demo. But if you wanted to get a lot more content from the Wikipedia page, then you would put a thousand or 10,000. Again, it will take longer to run and you may get rate limited faster because you don't even need an API key for this. But again, just showing you a quick example. So these are two parameters and I believe there's a few more that you can pass here like the language, load all available metadata, et cetera. Okay, now that we have the API wrapper, we need to convert this to a tool. So we're gonna say the wiki tool is equal to the Wikipedia query run. And then all we're gonna do is just pass our API wrapper equal to our API wrapper. Now that's actually all we need for the tool. We don't need to wrap it in a custom tool. We can just pass this as a tool directly to LangChain. So what I'm gonna do now is import the wiki tool. So say wiki underscore tool. And then I'm gonna go here to my tools list, bring in the wiki tool. And now we can run this and let's see what we get. And if it's able to use Wikipedia. So I'm gonna say the same thing, you know, sharks, or let's go hammerhead sharks. Okay, let's see what we get. So you can see it's using Wikipedia, looking up the hammerhead shark, and then it's using search. Yes, search hammerhead shark research latest findings. And then it gives us the response and it tells us that it used these two tools. Sweet. Okay, last thing, I'm gonna show you how we make our own custom tool so we can save this to a file. So in order to save this to a file, we can actually just write our own Python function. So actually let's go up to the top here. And this function or any function for that matter can be wrapped as a tool. So we're just gonna make a function called save. Actually, I'm gonna save some time because I don't think you guys need to watch me write this out. I'm just gonna copy it in called save to TXT. We'll take in some data and we'll take in a file name. Now it's important that you give the parameters a type here so that the model knows how to call this function. So make sure that you type them. In this case, I've typed it as a string. If it was a more advanced type, you'd want to include that as well. So what I'm doing is I'm just writing at the top of the paper in a research output, the timestamp, and then I'm gonna write the data. And that data is gonna be that Pydantic model, which you'll see in just one second. Okay, so that's my function. Now, once we have the function, we just need to wrap it as a tool. So to do that, we can say save underscore tool is equal to tool. And then we just do the exact same thing that we had here. So I'm just gonna copy this, paste it. I'm gonna change the function to be save to TXT. Notice I didn't call the function. I just wrote the name of it. And then for the name, we're just gonna say save text to file. Again, make sure we don't have any spaces. And then I will just kind of bring in this description, you know, save structured research data to a text file. So it knows what this is doing. That's as easy as it is to make your own custom tool. So if you want a tool that calls an API, for example, you can do that. Just write a Python function, wrap it as a tool, and you can pass as many of these to your agent that you want, and really get some advanced functionality here. So now we're gonna bring in the tool. So same thing, we're gonna bring in the save to TXT. That's what I called it, right? No, sorry, save tool. Okay, and then we're going to put that in the list, save tool, and then we can start using the save tool. Now, the only thing is we just need to instruct the model to save this to a file. So I'm gonna go Python main.py. I'm gonna say, let's research, I don't know, what do we want to research? You know, South East Asia population or something, and say save to a file. Okay, and then it should use kind of all of the tools and save this to a file. Let's see if it does that, and just give this a second to run. So you can see it just used Wikipedia. It's using, what else? South East Asia, okay. I think it's just my terminals cutting off a little bit here, and it used the save text to file, and then finished the chain, gave us the output, and now you can see we have our research output TXT file, and inside of here, we get our timestamp, we get our topic, and we get all of this information. You can see the region has relatively young population, blah, blah, blah, blah, blah, and goes through kind of all of the details. So there you go. We have just completed the project and built an agent from scratch in Python that has access to various tools. This is super cool. We are really just scratching the surface with what is possible here. I just wanted to give you a video that kind of overviewed the main components, the main topics that make up like 80% of the Asian applications, and this really does get you quite far and allows you to build some really cool stuff. So I will leave the code link here in the description in case you want to check out the GitHub, which will have all of this content. You can just copy it and do whatever you want with it. If you enjoyed the video, make sure to leave a like, subscribe to the channel, and I will see you in the next one. This course from Harvard University explores the concepts and algorithms at the foundation of modern artificial intelligence, diving into the ideas that give rise to technologies like game-playing engines, handwriting recognition, and machine translation. You'll gain exposure to the theory behind graph search algorithms, classification, optimization, reinforcement learning, and other topics in artificial intelligence and machine learning. Brian Yu teaches this course. Hello, world. This is CS50, and this is an introduction to artificial intelligence with Python with CS50's own Brian Yu. This course picks up where CS50 itself leaves off and explores the concepts and algorithms at the foundation of modern AI. We'll start with a look at how AI can search for solutions to problems, whether those problems are learning how to play a game or trying to find driving directions to a destination. We'll then look at how AI can represent information, both knowledge that our AI is certain about, but also information and events about which our AI might be uncertain, learning how to represent that information, but more importantly, how to use that information to draw inferences and new conclusions as well. We'll explore how AI can solve various types of optimization problems, trying to maximize profits or minimize costs or satisfy some other constraints before turning our attention to the fast-growing field of machine learning, where we won't tell our AI exactly how to solve a problem, but instead give our AI access to data and experiences so that our AI can learn on its own how to perform these tasks. In particular, we'll look at neural networks, one of the most popular tools in modern machine learning, inspired by the way that human brains learn and reason as well before finally taking a look at the world of natural language processing so that it's not just us humans learning to learn how artificial intelligence is able to speak, but also AI learning how to understand and interpret human language as well. We'll explore these ideas and algorithms, and along the way, give you the opportunity to build your own AI programs to implement all of this and more. This is CS50. All right, welcome, everyone, to an introduction to artificial intelligence with Python. My name is Brian Yu, and in this class, we'll explore some of the ideas and techniques and algorithms that are at the foundation of artificial intelligence. Now, artificial intelligence covers a wide variety of types of techniques, and we'll look at some of them in a little bit more detail. So, let's get started. So, let's start with the first one, which is artificial intelligence. So, artificial intelligence is a way that covers a wide variety of types of techniques. Anytime you see a computer do something that appears to be intelligent or rational in some way, like recognizing someone's face in a photo or being able to play a game better than people can or being able to understand human language when we talk to our phones and they understand what we mean and are able to respond back to us, these are all examples of AI or artificial intelligence. And in this class, we'll explore some of the ideas that make that AI possible. So, we'll begin our conversations with search, the problem of we have an AI, and we would like the AI to be able to search for solutions to some kind of problem, no matter what that problem might be, whether it's trying to get driving directions from point A to point B, or trying to figure out how to play a game, given a tic-tac-toe game, for example, figuring out what move it ought to make. After that, we'll take a look at knowledge. Ideally, we want our AI to be able to know information, to be able to represent that information, and more importantly, to be able to draw inferences from that information, to use the information it knows and draw additional conclusions. So, we'll talk about how AI can be programmed in order to do just that. Then we'll explore the topic of uncertainty, talking about ideas of what happens if a computer isn't sure about a fact, but maybe is only sure with a certain probability. So, we'll talk about some of the ideas behind probability and how computers can begin to deal with uncertain events, in order to be a little bit more intelligent in that sense as well. After that, we'll turn our attention to optimization, problems of when the computer is trying to optimize for some sort of goal, especially in a situation where there might be multiple ways that a computer might solve a problem, but we're looking for a better way, or potentially the best way, if that's at all possible. Then we'll take a look at machine learning, or learning more generally, and looking at how, when we have access to data, our computers can be programmed to be quite intelligent by learning from data and learning from experience, being able to perform a task better and better based on greater access to data. So, your email, for example, or your email inbox, somehow knows which of your emails are good emails and which of your emails are spam. These are all examples of computers being able to learn from past experiences and past data. We'll take a look, too, at how computers are able to draw inspiration from human intelligence, looking at the structure of the human brain and how neural networks can be a computer analog to that sort of idea, and how by taking advantage of a certain type of structure of a computer program, we can write neural networks that are able to perform tasks very, very effectively. And then finally, we'll turn our attention to language. Not programming languages, but human languages that we speak every day. And taking a look at the challenges that come about as a computer tries to understand natural language and how it is that some of the natural language processing that occurs in modern artificial intelligence can actually work. But today, we'll begin our conversation with search, this problem of trying to figure out what to do when we have some sort of situation that the computer is in, some sort of environment that an agent is in, so to speak, and we would like for that agent to be able to somehow look for a solution to that problem. Now, these problems can come in any number of different types of formats. One example, for instance, might be something like this classic 15 puzzle with the sliding tiles that you might have seen, where you're trying to slide the tiles in order to make sure that all the numbers line up in order. This is an example of what you might call a search problem. The 15 puzzle begins in an initially mixed-up state, which is a way of finding moves to make in order to return the puzzle to its solved state. But there are similar problems that you can frame in other ways. Trying to find your way through a maze, for example, is another example of a search problem. You begin in one place, you have some goal of where you're trying to get to, and you need to figure out the correct sequence of actions that will take you from that initial state to the goal. And while this is a little bit abstract, anytime we talk about maze-solving in this class, you can translate it to something a little more real-world, like finding directions. If you ever wonder how Google Maps is able to figure out what is the best way for you to get from point A to point B, and what turns to make at what time, depending on traffic, for example, it's often some sort of search algorithm. You have an AI that is trying to get from an initial position to some sort of goal by taking some sequence of actions. So we'll start our conversations today by thinking about these types of search problems and what goes into solving a search problem like this in order for an AI to be able to find a good solution. In order to do so, though, we're going to need to introduce a little bit of terminology, some of which I've already used. But the first term we'll need to think about is an agent. An agent is just some entity that perceives its environment and somehow is able to perceive the things around it and act on that environment in some way. So in the case of the driving directions, your agent might be some representation of a car that is trying to figure out what actions to take in order to arrive at a destination. In the case of the 15 puzzle with the sliding tiles, the agent might be the AI or the person that is trying to solve that puzzle, to try and figure out what tiles to move in order to get to that solution. Next, we introduce the idea of a state. A state is just some configuration of the agent in its environment. So in the 15 puzzle, for example, any state might be any one of these three, for example. A state is just some configuration of the tiles, and each of these states is different and is going to require a slightly different solution. A different sequence of actions will be needed in each of these states. In order to get from this initial state to the goal, which is where we're trying to get. So the initial state, then, what is that? The initial state is just the state where the agent begins. It is one such state where we're going to start from, and this is going to be the starting point for our search algorithm, so to speak. We're going to begin with this initial state and then start to reason about it, to think about what actions might we apply to that initial state in order to figure out how to get from this initial state to the goal, which is where we're trying to get. So the initial state, then, what is that? The initial state in order to figure out how to get from the beginning to the end, from the initial position to whatever our goal happens to be. And how do we make our way from that initial position to the goal? Well, ultimately, it's via taking actions. Actions are just choices that we can make in any given state. And in AI, we're always going to try to formalize these ideas a little bit more precisely, such that we can program them a little bit more mathematically, so to speak. So this will be a recurring theme, and we can more precisely define actions as a function. We're going to effectively define a function called actions that takes an input, S, where S is going to be some state that exists inside of our environment, and actions of S is going to take this state as input and return as output the set of all actions that can be executed in that state. And so it's possible that some actions are only valid in certain states and not in other states. And we'll see examples of that soon, too. So in the case of the 15 puzzle, for example, there are generally going to be four possible actions that we can do most of the time. We can slide a tile to the right, slide a tile to the left, slide a tile up, or slide a tile down, for example. And those are going to be the actions that are available to us. So somehow our AI, our program, needs some encoding of the state, which is often going to be in some numerical format, and some encoding of these actions. But it also needs some encoding of the relationship between these things. How do the states and actions relate to one another? And in order to do that, we'll introduce to our AI a transition model, which will be a description of what state we get after we perform some available action in some other state. And again, we can be a little bit more precise about this, define this transition model a little bit more formally, again, as a function. The function is going to be a function called result that this time takes two inputs. Input number one is S, some state, and input number two is A, some action. And the output of this function result is it is going to give us the state that we get after we perform action A in state S. So let's take a look at an example to see more precisely what this actually means. Here's an example of a state, of the 15 puzzle, for example. And here's an example of an action, sliding a tile to the right. What happens if we pass these as inputs to the result function? Again, the result function takes this board, and it takes an action as a second input. And of course here, I'm describing things visually so that you can see visually what the state is and what the action is. In a computer, you might represent one of these actions as just some number that represents the action, or if you're familiar with enums that allow you to enumerate multiple possibilities, it might be something like that. And the state might just be represented as an array or two-dimensional array of all of these numbers that exist. But here, we're going to show it visually, just so you can see. When we take this state and this action, pass it into the result function, the output is a new state, the state we get after we take a tile and slide it to the right, and this is the state we get as a result. If we had a different action and a different state, for example, and passed that into the result function, we'd get a different answer altogether. So the result function needs to take care of figuring out how to take a state and take an action and get what results, and this is going to be our transition model that describes how it is that states and actions are related to each other. If we take this transition model and think about it more generally and across the entire problem, we can form what we might call a state space, the set of all of the states we can get from the initial state via any sequence of actions by taking 0 or 1 or 2 or more actions in addition to that, so we can draw a diagram that looks something like this, where every state is represented here by a game board, and there are arrows that connect every state to every other state we can get to from that state, and the state space is much larger than what you see just here. This is just a sample of what the state space might actually look like. And in general, across many search problems, whether they're this particular 15 puzzle or driving directions or something else, the state space is going to look something like this. We have individual states and arrows that are connecting them, and oftentimes, just for simplicity, we'll simplify our representation of this entire thing as a graph, some sequence of nodes and edges that connect nodes. But you can think of this more abstract representation as the exact same idea. Each of these little circles or nodes is going to represent one of the states inside of our problem, and the arrows here represent the actions that we can take in any particular state, taking us from one particular state to another state, for example. All right. So now we have this idea of nodes that are representing these states, actions that can take us from one state to another, and a transition model that defines what happens after we take a particular action. So the next step we need to figure out is how we know when the AI is done solving the problem. The AI needs some way to know when it gets to the goal that it's found the goal. So the next thing we'll need to encode into our artificial intelligence is a goal test, some way to determine whether a given state is a goal state. In the case of something like driving directions, it might be pretty easy. In a state that corresponds to whatever the user typed in as their intended destination, well, then you know you're in a goal state. In the 15 puzzle, it might be checking the numbers to make sure they're all in ascending order. But the AI needs some way to encode whether or not any state they happen to be in is a goal. And some problems might have one goal, like a maze where you have one initial position and one ending position, and that's the goal. In other, more complex problems, you might imagine that there are multiple possible goals, that there are multiple ways to solve a problem, and we might not care which one the computer finds, as long as it does find a particular goal. However, sometimes a computer doesn't just care about finding a goal, but finding a goal well, or one with a low cost. And it's for that reason that the last piece of terminology that we use to define these search problems is something called a path cost. You might imagine that in the case of driving directions, it would be pretty annoying if I said I wanted directions from point A to point B, and the route that Google Maps gave me was a long route with lots of detours that were unnecessary, that took longer than it should have for me to get to that destination. And it's for that reason that when we're formulating search problems, we'll often give every path some sort of numerical cost, some number telling us how expensive it is to take this particular option, and then tell our AI that instead of just finding a solution, some way of getting from the initial state to the goal, we'd really like to find one that minimizes this path cost, that is less expensive, or takes less time, or minimizes some other numerical value. We can represent this graphically, if we take a look at this graph again, and imagine that each of these arrows, each of these actions that we can take from one state to another state has some sort of number associated with it, that number being the path cost of this particular action, where some of the cost for any particular action might be more expensive than the cost for some other action, for example. Although this will only happen in some sorts of problems. In other problems, we can simplify the diagram and just assume that the cost of any particular action is the same. And this is probably the case in something like the 15 puzzle, for example, where it doesn't really make a difference whether I'm moving right or moving left, the only thing that matters is the total number of steps that I have to take to get from point A to point B, and each of those steps is of equal cost. We can just assume it's of some constant cost like 1. And so this now forms the basis for what we might consider to be a search problem. A search problem has some sort of initial state, some place where we begin, some sort of action that we can take, or multiple actions that we can take in any given state, and it has a transition model, some way of defining what happens when we go from one state and take one action, what state do we end up with as a result. In addition to that, we need some goal test to know whether or not we've reached a goal, and then we need a path test function that tells us for any particular path, by following some sequence of actions, how expensive is that path, what is its cost in terms of money or time or some other resource that we are trying to minimize our usage of. And the goal ultimately is to find a solution, where a solution in this case is just some sequence of actions that will take us from the initial state to the goal state and ideally, we'd like to find not just any solution, but the optimal solution, which is a solution that has the lowest path cost among all of the possible solutions. And in some cases, there might be multiple optimal solutions, but an optimal solution just means that there is no way that we could have done better in terms of finding that solution. So now we've defined the problem, and now we need to begin to figure out how it is that we're going to solve this kind of search problem. And in order to do so, you'll probably imagine that our computer is going to need to represent a whole bunch of data about this particular problem. We need to represent a whole bunch of data about where we are in the problem, and we might need to be considering multiple different options at once. And oftentimes when we're trying to package a whole bunch of data related to a state together, we'll do so using a data structure that we're going to call a node. A node is a data structure that is just going to keep track of a variety of different values, and specifically in the case of a search problem, it's going to keep track of these four values in particular. Every node is going to keep track of a state, the state we're currently on, and every node is also going to keep track of a parent, a parent being the state before us, or the node that we used in order to get to this current state. And this is going to be relevant, because eventually, once we reach the goal node, once we get to the end, we want to know what sequence of actions we used in order to get to that goal. And the way we'll know that is by looking at these parents to keep track of what led us to the goal, and what led us to that state, and what led us to the state before that, so on and so forth, and then tracking our way to the beginning so that we know the entire sequence of actions we needed in order to get from the beginning to the end. The node is also going to keep track of what action we took in order to get from the parent to the current state, and the node is also going to keep track of a path cost. In other words, it's going to keep track of the number that represents how long it took to get from the initial state to the state that we currently happen to be at. And we'll see why this is relevant as we start to talk about some of the optimizations that we can make in terms of these search problems more generally. So this is the data structure that we're going to use in order to solve the problem, and now let's talk about the approach. How might we actually begin to solve the problem? Well, as you might imagine, what we're going to do is we're going to start at one particular state, and we're just going to explore from there. The intuition is that from a given state, we have multiple options that we can take, and we're going to explore those options. And once we explore those options, the intuition is that more options than that are going to make themselves available. And we're going to consider all of the available options to be stored inside of a single data structure that we'll call the frontier. The frontier is going to represent all of the things that we could explore next that we haven't yet explored or visited. So in our approach, we're going to begin the search algorithm by starting with a frontier that just contains one state. The frontier is going to contain the initial state, because at the beginning, that's the only state we know about. That is the only state that exists. And then, our search algorithm is effectively going to follow a loop. We're going to repeat some process again and again and again. The first thing we're going to do is if the frontier is empty, then there's no solution, and we can report that there is no way to get to the goal. And that's certainly possible. There are certain types of problems that an AI might try to explore and realize that there is no way to solve that problem, and that's useful information for humans to know as well. So if ever the frontier is empty, that means there's nothing left to explore, and we haven't yet found a solution, so there is no solution. There's nothing left to explore. Otherwise, what we'll do is we'll remove a node from the frontier. So right now, at the beginning, the frontier just contains one node, representing the initial state. But over time, the frontier might grow. It might contain multiple states. And so here, we're just going to remove a single node from that frontier. If that node happens to be a goal, then we found a solution. So we remove a node from the frontier and ask ourselves, is this the goal? And we do that by applying the goal test that we talked about earlier, asking if we're at the destination, or asking if all the numbers of the 15 puzzle happen to be in order. So if the node contains the goal, we found a solution, great, we're done. And otherwise, what we'll need to do is we'll need to expand the node. And this is a term of art in artificial intelligence. To expand the node just means to look at all of the neighbors of that node. In other words, consider all of the possible actions that I could take from the state that this node is representing, and what nodes could I get to from there. We're going to take all of those nodes, the next nodes that I can get to from this current one I'm looking at, and add those to the frontier. And then we'll repeat this process. So at a very high level, the idea is we start with a frontier that contains the initial state, and we're constantly removing a node from the frontier, looking at where we can get to next, and adding those nodes to the frontier, repeating this process over and over until either we remove a node from the frontier, and it contains a goal, meaning we've solved the problem, or we run into a situation where the frontier is empty, at which point we're left with no solution. So let's actually try and take this pseudocode, put it into practice by taking a look at an example of a sample search problem. So right here, I have a sample graph, A is connected to B via this action, B is connected to node C, and D is C is connected to E, D is connected to F, and what I'd like to do is have my AI find a path from A to E. We want to get from this initial state to this goal state. So how are we going to do that? Well, we're going to start with a frontier that contains the initial state. This is going to represent our frontier. So our frontier initially will just contain A, that initial state where we're going to begin. And now we'll repeat this process. If the frontier is empty, no solution, that's not a problem because the frontier is not empty. So we'll remove a node from the frontier as the one to consider next. There's only one node in the frontier, so we'll go ahead and remove it from the frontier, but now A, this initial node, this is the node we're currently considering. We follow the next step, we ask ourselves, is this node the goal? No, it's not. A is not the goal, E is the goal. So we don't return the solution. So instead, we go to this last step, expand the node, and add the resulting nodes to the frontier. What does that mean? Well, it means take this state A, and consider where we can get to next. And after A, what we can get to next is only B. So that's what we get when we expand A, we find B, and we add B to the frontier, and now B is in the frontier, and we repeat the process again. We say, alright, the frontier is not empty, so let's remove B from the frontier. B is now the node that we're considering. We ask ourselves, is B the goal? No, it's not. So we go ahead and expand B, and add its resulting nodes to the frontier. What happens when we expand B? In other words, what nodes can we get to from B? Well, we can get to C and D, so we'll go ahead and add C and D from the frontier, and now we have two nodes in the frontier, C and D, and we repeat the process again. We remove a node from the frontier, for now I'll do so arbitrarily, just by picking C. We'll see why later, how choosing which node you remove from the frontier is actually quite an important part of the algorithm. For now, I'll arbitrarily remove C, say it's not the goal, so we'll add E, the next one to the frontier, then let's say I remove E from the frontier, and now I check, I'm currently looking at state E, is it a goal state? It is, because I'm trying to find a path from A to E, so I would return the goal, and that now would be the solution, that I'm now able to return the solution, and I've found a path from A to E. So this is the general idea, the general approach of this search algorithm, to follow these steps, constantly removing nodes from the frontier, until we're able to find a solution. So the next question you might reasonably ask is what could go wrong here? What are the potential problems with an approach like this? And here's one example of a problem that could arise from this sort of approach. Imagine this same graph, same as before, with one change. The change being now, instead of just an arrow from A to B, we also have an arrow from B to A, meaning we can go in both directions.

And this is true in something like the 15 puzzle, where when I slide a tile to the right, I could then slide a tile to the left to get back to the original position. I could go back and forth between A and B, and that's what these double arrows symbolize. The idea that from one state, I can get to another, and then I can get back. And that's true in many search problems. What's going to happen if I try to apply the same approach now? Well, I'll begin with A, same as before, and I'll remove A from the frontier. And then I'll consider where I can get to from A. And after A, the only place I can get to is B. So B goes into the frontier. Then I'll say, all right, let's take a look at B. That's the only thing left in the frontier. Where can I get to from B? Before, it was just C and D. But now, because of that reverse arrow, I can get to A or C or D. So all three, A, C, and D, all of those now go into the frontier. They are places I can get to from B. And now I remove one from the frontier, and maybe I'm unlucky, and maybe I pick A. And now I'm looking at A again. And I consider where can I get to from A, and from A, well, I can get to B. And now we start to see the problem. But if I'm not careful, I go from A to B and then back to A and then to B again. And I could be going in this infinite loop where I never make any progress because I'm constantly just going back and forth between two states that I've already seen. So what is the solution to this? We need some way to deal with this problem. And the way that we can deal with this problem is by somehow keeping track of what we've already explored. And the logic is going to be, well, if we've already explored the state, there's no reason to go back to it. Once we've explored a state, don't go back to it. Don't bother adding it to the frontier. There's no need to. So here's going to be our revised approach, a better way to approach this sort of search problem. And it's going to look very similar, just with a couple of modifications. We'll start with a frontier that contains the initial state, same as before. But now we'll start with another data structure, which will just be a set of nodes that we've already explored. So what are the states we've explored? Initially, it's empty. We have an empty explored set. And now we repeat. If the frontier is empty, no solution, same as before. We remove a node from the frontier. We check to see if it's a goal state, return the solution. None of this is any different so far. But now what we're going to do is we're going to add the node to the explored state. So if it happens to be the case that we remove a node from the frontier and it's not the goal, we'll add it to the explored set so that we know we've already explored it. We don't need to go back to it again if it happens to come up later. And then the final step, we expand the node and we add the resulting nodes to the frontier. But before, we just always added the resulting nodes to the frontier. We're going to be a little clever about it this time. We're only going to add the nodes to the frontier if they aren't already in the frontier and if they aren't already in the explored set. So we'll check both the frontier and the explored set, make sure that the node isn't already in one of those two, and so long as it isn't, then we'll go ahead and add it to the frontier, but not otherwise. And so that revised approach is ultimately what's going to help make sure that we don't go back and forth between two nodes. Now the one point that I've kind of glossed over here so far is this step here, removing a node from the frontier. Before I just chose arbitrarily, like let's just remove a node and that's it. But it turns out it's actually quite important how we decide to structure our frontier, how we add and how we remove our nodes. The frontier is a data structure, and we need to make a choice about in what order are we going to be removing elements. And one of the simplest data structures for adding and removing elements is something called a stack. And a stack is a data structure that is a last in, first out data type, which means the last thing that I add to the frontier is going to be the first thing that I remove from the frontier. So the most recent thing to go into the stack, or the frontier in this case, is going to be the node that I explore. So let's see what happens if I apply this stack-based approach to something like this problem, finding a path from A to E. What's going to happen? Well, again, we'll start with A, and we'll say, all right, let's go ahead and look at A first. And then notice this time we've added A to the explored set. A is something we've now explored. We have this data structure that's keeping track. We then say from A, we can get to B. And, all right, from B, what can we do? Well, from B, we can explore B and get to both C and D. So we added C and then D. So now when we explore a node, we're going to treat the frontier as a stack, last in, first out. D was the last one to come in, so we'll go ahead and explore that next and say, all right, where can we get you from D? Well, we can get to F. It's all right, we'll explore it, put F into the frontier. And now, because the frontier is a stack, F is the most recent thing that's gone in the stack, so F is what we'll explore next. We'll explore F and say, all right, where can we get you from F? Well, we can't get anywhere, so nothing gets added to the frontier. So now what was the new most recent thing added to the frontier? Well, it's now C, the only thing left in the frontier. We'll explore that, from which we can see, all right, from C we can get to E, E goes into the frontier, and then we say, all right, let's look at E, and E is now the solution, and now we've solved the problem. So when we treat the frontier like a stack, a last in, first out data structure, that's the result we get. We go from A to B to D to F, and then we sort of backed up and went down to C and then E. And it's important to get a visual sense for how this algorithm is working. We went very deep in this search tree, so to speak, all the way until the bottom, where we hit a dead end, and then we effectively backed up and explored this other route that we didn't try before. And it's this going very deep in the search tree idea, this way the algorithm ends up working when we use a stack, that we call this version of the algorithm depth-first search. Depth-first search is the search algorithm where we always explore the deepest node in the frontier. We keep going deeper and deeper through our search tree, and then if we hit a dead end, we back up and we try something else instead. But depth-first search is just one of the possible search options that we can use. It turns out that there's another algorithm called breadth-first search, which behaves very similarly to depth-first search with one difference. Instead of always exploring the deepest node in the search tree, the way that depth-first search does, breadth-first search is always going to explore the shallowest node in the frontier. So what does that mean? Well, it means that instead of using a stack, which depth-first search, or DFS, used, where the most recent item added to the frontier is the one we'll explore next, in breadth-first search, or BFS, we'll instead use a queue, where a queue is a first-in, first-out data type, where the very first thing we add to the frontier is the first one we'll explore, and they effectively form a line, or a queue, where the earlier you arrive in the frontier, the earlier you get explored. So what would that mean for the same exact problem, finding a path from A to E? Well, we start with A, same as before. Then we'll go ahead and have explored A, and say, where can we get to from A? Well, from A we can get to B, same as before. From B, same as before, we can get to C and D, so C and D get added to the frontier. This time, though, we added C to the frontier before D, so we'll explore C first. So C gets explored. And from C, where can we get to? Well, we can get to E. So E gets added to the frontier, but because D was explored before E, we'll look at D next. So we'll explore D and say, where can we get to from D? We can get to F. And only then will we say, all right, now we can get to E. And so what breadth-first search, or BFS, did, is we started here, we looked at both C and D, and then we looked at E. Effectively, we're looking at things one away from the initial state, then two away from the initial state, and only then things that are three away from the initial state, unlike depth-first search, which just went as deep as possible into the search tree until it hit a dead end, and then ultimately had to back up. So these now are two different search algorithms that we could apply in order to try and solve a problem. And let's take a look at how these would actually work in practice with something like maze solving, for example. So here's an example of a maze. These empty cells represent places where our agent can move. These darkened gray cells represent walls that the agent can't pass through. And ultimately, our agent, our AI, is going to try to find a way to get from position A to position B via some sequence of actions, where those actions are left, right, up, and down. What will depth-first search do in this case? Well, depth-first search will just follow one path. If it reaches a fork in the road where it has multiple different options, depth-first search is just, in this case, going to choose one. That isn't a real preference. But it's going to keep following one until it hits a dead end. And when it hits a dead end, depth-first search effectively goes back to the last decision point and tries the other path, fully exhausting this entire path. And when it realizes that, okay, the goal is not here, then it turns its attention to this path. It goes as deep as possible. When it hits a dead end, it backs up and then tries this other path, keeps going as deep as possible down one particular path. And when it realizes that that's a dead end, then it'll back up and then ultimately find its way to the goal. And maybe you got lucky, and maybe you made a different choice earlier on, but ultimately this is how depth-first search is going to work. It's going to keep following until it hits a dead end, and when it hits a dead end, it backs up and looks for a different solution. And so one thing you might reasonably ask is, is this algorithm always going to work? Will it always actually find a way to get from the initial state to the goal? And it turns out that as long as our maze is finite, as long as there are only finitely many spaces where we can travel, then yes, depth-first search is going to find a solution because eventually it'll just explore everything. If the maze happens to be infinite and there's an infinite state space, which does exist in certain types of problems, then it's a slightly different story. But as long as our maze has finitely many squares, we're going to find a solution. The next question, though, that we want to ask is, is it going to be a good solution? Is it the optimal solution that we can find? And the answer there is not necessarily. And let's take a look at an example of that. In this maze, for example, we're again trying to find our way from A to B. And you notice here there are multiple possible solutions. We could go this way, or we could go up in order to make our way from A to B. Now, if we're lucky, depth-first search will choose this way and get to B, but there's no reason necessarily why depth-first search would choose between going up or going to the right. It's sort of an arbitrary decision point because both are going to be added to the frontier. And ultimately, if we get unlucky, depth-first search might choose to explore this path first because it's just a random choice at this point. It'll explore, explore, explore, and it'll eventually find the goal, this particular path, when in actuality there was a better path, there was a more optimal solution that used fewer steps, assuming we're measuring the cost of a solution based on the number of steps that we need to take. So depth-first search, if we're unlucky, might end up not finding the best solution when a better solution is available. So that's DFS, depth-first search. How does BFS, or breadth-first search, compare? How would it work in this particular situation? Well, the algorithm is going to look very different visually in terms of how BFS explores. Because BFS looks at shallower nodes first, the idea is going to be, BFS will first look at all of the nodes that are one away from the initial state. Look here and look here, for example, just at the two nodes that are immediately next to this initial state. Then it'll explore nodes that are two away, looking at this state and that state, for example. Then it'll explore nodes that are three away, this state and that state. Whereas depth-first search just picked one path and kept following it, breadth-first search, on the other hand, is taking the option of exploring all of the possible paths kind of at the same time, bouncing back between them, looking deeper and deeper at each one, but making sure to explore the shallower ones, or the ones that are closer to the initial state, earlier. So we'll keep following this pattern, looking at things that are four away, looking at things that are five away, looking at things that are six away, until eventually we make our way to the goal. And in this case, it's true, we had to explore some states that ultimately didn't lead us anywhere, but the path that we found to the goal was the optimal path. This is the shortest way that we could get to the goal. And so what might happen then in a larger maze? Well, let's take a look at something like this and how breadth-first search is going to behave. Well, breadth-first search, again, will just keep following the states until it receives a decision point. It could go either left or right. And while DFS just picked one and kept following that until it hit a dead end, BFS, on the other hand, will explore both. It'll say, look at this node, then this node, and it'll look at this node, then that node, so on and so forth. And when it hits a decision point here, rather than pick one left or two right and explore that path, it'll again explore both, alternating between them, going deeper and deeper. We'll explore here, and then maybe here and here. Explore here, and then keep going. Explore here, and slowly make our way, you can visually see, further and further out. Once we get to this decision point, we'll explore both up and down until ultimately we make our way to the goal. And what you'll notice is, yes, breadth-first search did find our way from A to B by following this particular path, but it needed to explore a lot of states in order to do so. So we see some trade-offs here between DFS and BFS. But in DFS, there may be some cases where there is some memory savings as compared to a breadth-first approach, where breadth-first search in this case had to explore a lot of states, but maybe that won't always be the case. So now let's actually turn our attention to some code and look at the code that we could actually write in order to implement something like depth-first search or breadth-first search in the context of solving a maze, for example. So I'll go ahead and go into my terminal, and what I have here inside of maze.py is an implementation of this same idea of maze solving. I've defined a class called Node that in this case is keeping track of the state, the parent, in other words the state before the state, and the action. In this case, we're not keeping track of the path cost because we can calculate the cost of the path at the end after we've found our way from the initial state to the goal. In addition to this, I've defined a class called a Stack Frontier, and if unfamiliar with a class, a class is a way for me to define a way to generate objects.