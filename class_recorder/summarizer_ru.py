"""Russian LLM summarization with translation to English."""
import json
from collections import OrderedDict
from pathlib import Path

from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from .utils import (
    build_frontmatter,
    chunk_text,
    clean_transcript_text,
    extract_date_fragment,
)

# ==================== RUSSIAN PROMPTS ====================

NARRATIVE_CHUNK_PROMPT_RU = '''Ð¢Ñ‹ Ð»Ð¸Ñ‡Ð½Ñ‹Ð¹ Ñ€ÐµÐ¿ÐµÑ‚Ð¸Ñ‚Ð¾Ñ€ Ð­Ð¼Ð¸Ñ€Ñ…Ð°Ð½Ð°, Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÑ‹Ð²Ð°ÑŽÑ‰Ð¸Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ð»ÐµÐºÑ†Ð¸Ð¸ {chunk_index}/{total_chunks}.

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐžÐ§Ð˜Ð©Ð•ÐÐÐ«Ð™ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ð´Ð»Ñ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð¾Ðº, Ð½Ð¾ ÑÐ²ÐµÑ€ÑÐ¹ÑÑ Ñ Ð¡Ð«Ð Ð«Ðœ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð²ÑÐµ Ñ„Ð°ÐºÑ‚Ñ‹.

ÐžÑ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸:
"""{cleaned_chunk}"""

Ð¡Ñ‹Ñ€Ð¾Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÑÐ¿Ñ€Ð°Ð²ÐºÐ¸):
"""{raw_chunk}"""

ÐŸÑ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ñ€Ð°Ð·Ð´ÐµÐ»Ð¾Ð²: {prior_topics}
{context_instruction}

ÐŸÑ€Ð°Ð²Ð¸Ð»Ð°:
- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÑÑ‚Ð¾Ð³Ð¾ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð°.
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ Ð² ÑÑ‚Ð¸Ð»Ðµ `## [HH:MM:SS] Ð¢ÐµÐ¼Ð°`, ÐºÐ¾Ð³Ð´Ð° ÐµÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸ Ð¸Ð»Ð¸ Ð¿Ð¾Ð´ÑÐºÐ°Ð·ÐºÐ¸; Ð¸Ð½Ð°Ñ‡Ðµ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ð¹ Ð¾Ð¿Ð¸ÑÐ°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸.
- Ð’Ñ‹Ð´ÐµÐ»ÑÐ¹ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ/Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ñ‹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ Ð²Ñ‹Ð½Ð¾ÑÐ¾Ðº `> [!important]`.
- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹, Ð¿Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ð·Ð°Ð´Ð°Ñ‡ Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹-Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹.
- Ð•ÑÐ»Ð¸ Ð°ÑƒÐ´Ð¸Ð¾ Ð½ÐµÑ‡ÐµÑ‚ÐºÐ¾Ðµ, Ð¿Ð¸ÑˆÐ¸ `[Ð½ÐµÑ€Ð°Ð·Ð±Ð¾Ñ€Ñ‡Ð¸Ð²Ð¾Ðµ Ð°ÑƒÐ´Ð¸Ð¾]` Ð²Ð¼ÐµÑÑ‚Ð¾ Ð´Ð¾Ð³Ð°Ð´Ð¾Ðº.
- ÐÐµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð», ÑƒÐ¶Ðµ Ð¾Ñ…Ð²Ð°Ñ‡ÐµÐ½Ð½Ñ‹Ð¹ Ð² Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð°Ñ….

Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Markdown Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð°.'''

GUIDE_CHUNK_PROMPT_RU = '''Ð¢Ñ‹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº Ð¿Ð¾ ÑƒÑ‡ÐµÐ±Ðµ Ð­Ð¼Ð¸Ñ€Ñ…Ð°Ð½Ð°. Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð·Ð°Ð¼ÐµÑ‚ÐºÐ¸ Ð¸Ð· Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð° {chunk_index}/{total_chunks}.

Ð’Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ:
- Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð°Ñ€Ñ€Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚:
"""{structured_chunk}"""
- ÐžÑ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸:
"""{cleaned_chunk}"""
- Ð¡Ñ‹Ñ€Ð¾Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ (ÑÐ¿Ñ€Ð°Ð²ÐºÐ°):
"""{raw_chunk}"""

ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ JSON (Ð±ÐµÐ· markdown Ð¾Ð³Ñ€Ð°Ð¶Ð´ÐµÐ½Ð¸Ð¹, Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°):
{{
  "mission_control": [],
  "key_concepts": [],
  "assignments": [],
  "study_theory": [],
  "study_practice": [],
  "study_admin": [],
  "exam_intel": [],
  "risk_followups": [],
  "next_moves": []
}}

ÐŸÑ€Ð°Ð²Ð¸Ð»Ð°:
- Ð’ÐºÐ»ÑŽÑ‡Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ²Ð½Ð¾ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹.
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ `Ñ‚ÐµÑ€Ð¼Ð¸Ð½ â€” Ð¾Ð±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ (Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¼ÐµÑ‚ÐºÐ°)` Ð´Ð»Ñ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¹, ÐºÐ¾Ð³Ð´Ð° ÐµÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸.
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ `- [ ] ...` Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ð½Ð¸Ð¹/ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ñ… Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð², Ð¸ `- ...` Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… ÑˆÐ°Ð³Ð¾Ð².
- Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°Ð¹ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð¼Ð°ÑÑÐ¸Ð²Ñ‹, ÐºÐ¾Ð³Ð´Ð° Ð² ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸ Ð½ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°.
'''

PROFESSOR_CHUNK_PROMPT_RU = '''Ð¢Ñ‹ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¾Ñ€ Ð­Ð¼Ð¸Ñ€Ñ…Ð°Ð½Ð°, Ð´Ð°ÑŽÑ‰Ð¸Ð¹ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ð´Ð»Ñ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð° {chunk_index}/{total_chunks}.

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð¼ÐµÑ‚ÐºÐ¸ Ð¿Ð»ÑŽÑ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð½ÑƒÑŽ/ÑÑ‹Ñ€ÑƒÑŽ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸.

Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚:
"""{structured_chunk}"""

ÐžÑ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸:
"""{cleaned_chunk}"""

Ð¡Ñ‹Ñ€Ð¾Ð¹ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ (ÑÐ¿Ñ€Ð°Ð²ÐºÐ°):
"""{raw_chunk}"""

Ð Ð°Ð½ÐµÐµ Ð¾Ñ…Ð²Ð°Ñ‡ÐµÐ½Ð½Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹: {prior_topics}

ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ð² Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð½Ð¾Ð¼ ÑÑ‚Ð¸Ð»Ðµ, Ð¿Ð¾Ð´Ñ‡ÐµÑ€ÐºÐ½Ð¸ Ð¸Ð½Ñ‚ÑƒÐ¸Ñ†Ð¸ÑŽ Ð¸ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´Ñ‹, Ð¸ Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°Ð¹ Ñ€ÐµÑ„Ð»ÐµÐºÑÐ¸Ð²Ð½Ñ‹Ð¼ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð¼ Ð¸Ð»Ð¸ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ ÑÐ°Ð¼Ð¾Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¾Ð¹, ÐºÐ¾Ð³Ð´Ð° ÑƒÐ¼ÐµÑÑ‚Ð½Ð¾. Ð’Ñ‹Ð²Ð¾Ð´Ð¸ Markdown Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð¼ `## Ð¡ÐµÐ³Ð¼ÐµÐ½Ñ‚ {chunk_index}`.'''

TRANSLATION_PROMPT = '''ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÑƒÑŽ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ Ð»ÐµÐºÑ†Ð¸Ð¸ Ñ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹.

Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸:
- Ð’ÑÐµ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹ Ñ‚Ð¾Ñ‡Ð½Ð¾
- Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ ÐµÑÑ‚ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾Ð¹ Ð´Ð»Ñ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð³Ð¾
- Ð’ÑÐµ Ð¸Ð¼ÐµÐ½Ð°, Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¹ Ð¸ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹

Ð¡Ñ‹Ñ€Ð°Ñ Ñ€ÑƒÑÑÐºÐ°Ñ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ:
"""{russian_text}"""

Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´, Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ².'''

# ==================== ENGLISH PROMPTS (for translated transcript) ====================

NARRATIVE_CHUNK_PROMPT_EN = '''You are Emirhan's private tutor rewriting lecture chunk {chunk_index}/{total_chunks}.

This is a TRANSLATED transcript from Russian. Use the CLEANED chunk for wording, but cross-check with the RAW chunk to preserve every fact.

Cleaned transcript chunk:
"""{cleaned_chunk}"""

Raw transcript chunk (reference only):
"""{raw_chunk}"""

Previous section headings: {prior_topics}
{context_instruction}

Rules:
- Preserve chronological order within this chunk.
- Use `## [HH:MM:SS] Topic` style headings when timestamps or cues are present; otherwise craft descriptive headings.
- Highlight definitions/formulas with `> [!important]` callouts.
- Keep examples, problem statements, and Q&A intact.
- If audio is unclear, write `[unclear audio]` rather than guessing.
- Do not repeat material already covered in earlier chunks.

Return only Markdown for this chunk.'''

GUIDE_CHUNK_PROMPT_EN = '''You are Emirhan's nerdy study buddy. Build actionable notes from chunk {chunk_index}/{total_chunks}.

This is from a TRANSLATED Russian lecture transcript.

Inputs:
- Structured narrative chunk:
"""{structured_chunk}"""
- Cleaned transcript chunk:
"""{cleaned_chunk}"""
- Raw transcript chunk (reference):
"""{raw_chunk}"""

Respond with JSON (no markdown fencing, no extra text):
{{
  "mission_control": [],
  "key_concepts": [],
  "assignments": [],
  "study_theory": [],
  "study_practice": [],
  "study_admin": [],
  "exam_intel": [],
  "risk_followups": [],
  "next_moves": []
}}

Rules:
- Only include facts explicitly stated.
- Use `term â€” explanation (timestamp)` for key concepts when timestamps exist.
- Use `- [ ] ...` for assignments/study items, and `- ...` for next moves.
- Return empty arrays when a category has no content.
'''

PROFESSOR_CHUNK_PROMPT_EN = '''You are Emirhan's friendly professor delivering a one-on-one recap for chunk {chunk_index}/{total_chunks}.

This is from a TRANSLATED Russian lecture transcript. Use the structured notes plus the cleaned/raw transcript for accuracy.

Structured chunk:
"""{structured_chunk}"""

Cleaned transcript chunk:
"""{cleaned_chunk}"""

Raw transcript chunk (reference):
"""{raw_chunk}"""

Previously covered topics: {prior_topics}

Explain the material conversationally, highlight intuition and transitions, and end with a reflective question or quick self-check when appropriate. Output Markdown with a heading `## Segment {chunk_index}`.'''


class RussianLLMSummarizer:
    """Handles Russian transcription processing with English translation."""
    
    def __init__(self, config):
        """
        Initialize with Russian-specific config.
        
        Args:
            config: Config object with russian_model and english_model settings
        """
        self.config = config
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=config.openrouter_api_key
        )
        
        # Separate models for Russian and English processing
        self.russian_model = config.get('summarization.russian_model', 'anthropic/claude-3.5-sonnet')
        self.english_model = config.get('summarization.english_model', 'anthropic/claude-3.5-haiku')
        
        self.max_tokens = config.get('summarization.max_tokens', 4000)
        self.temperature = config.get('summarization.temperature', 0.1)
        self.chunk_chars = int(config.get('summarization.chunk_chars', 6000))
        self.chunk_overlap = int(config.get('summarization.chunk_overlap_paragraphs', 5))

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=60)
    )
    def summarize(self, raw_transcript_text, course_folder, base_name, metadata=None, cleaned_transcript_text=None):
        """
        Generate both Russian and English summaries.
        
        Workflow:
        1. Process Russian transcript â†’ 3 Russian files
        2. Translate to English
        3. Process English transcript â†’ 3 English files
        
        Returns dict with paths to all 6 files.
        """
        print(f"\nðŸ‡·ðŸ‡º Generating Russian summaries with {self.russian_model}...")
        
        raw_transcript_text = raw_transcript_text or ""
        cleaned_transcript_text = cleaned_transcript_text or clean_transcript_text(raw_transcript_text)
        
        # ===== STEP 1: Generate Russian summaries =====
        russian_results = self._generate_language_summaries(
            raw_transcript_text,
            cleaned_transcript_text,
            language='ru',
            prompts={
                'narrative': NARRATIVE_CHUNK_PROMPT_RU,
                'guide': GUIDE_CHUNK_PROMPT_RU,
                'professor': PROFESSOR_CHUNK_PROMPT_RU
            },
            model=self.russian_model
        )
        
        # ===== STEP 2: Translate to English =====
        print(f"\nðŸ”„ Translating transcript to English with {self.russian_model}...")
        english_transcript = self._translate_to_english(cleaned_transcript_text)
        
        # Save translated transcript
        course_folder_path = Path(course_folder)
        date_str = extract_date_fragment(base_name)
        translated_path = course_folder_path / f"{date_str}-translated-en.txt"
        translated_path.write_text(english_transcript)
        print(f"âœ… English translation saved: {translated_path}")
        
        # ===== STEP 3: Generate English summaries =====
        print(f"\nðŸ‡¬ðŸ‡§ Generating English summaries with {self.english_model}...")
        english_results = self._generate_language_summaries(
            english_transcript,
            english_transcript,  # Already cleaned by translation
            language='en',
            prompts={
                'narrative': NARRATIVE_CHUNK_PROMPT_EN,
                'guide': GUIDE_CHUNK_PROMPT_EN,
                'professor': PROFESSOR_CHUNK_PROMPT_EN
            },
            model=self.english_model
        )
        
        # ===== STEP 4: Save all files =====
        course_name = metadata.get('course', 'Unknown') if metadata else 'Unknown'
        duration = metadata.get('duration', 0) if metadata else 0
        frontmatter = build_frontmatter(base_name, course_name, duration)
        
        # Russian files
        structured_ru = course_folder_path / f"{date_str}-structured-ru.md"
        guide_ru = course_folder_path / f"Guide {date_str}-ru.md"
        professor_ru = course_folder_path / f"Professor {date_str}-ru.md"
        
        structured_ru.write_text(self._compose_document(frontmatter, russian_results['structured']))
        guide_ru.write_text(self._compose_document(frontmatter, russian_results['guide']))
        professor_ru.write_text(self._compose_document(frontmatter, russian_results['professor']))
        
        # English files
        structured_en = course_folder_path / f"{date_str}-structured-en.md"
        guide_en = course_folder_path / f"Guide {date_str}-en.md"
        professor_en = course_folder_path / f"Professor {date_str}-en.md"
        
        structured_en.write_text(self._compose_document(frontmatter, english_results['structured']))
        guide_en.write_text(self._compose_document(frontmatter, english_results['guide']))
        professor_en.write_text(self._compose_document(frontmatter, english_results['professor']))
        
        print(f"\nâœ… Russian summaries saved:")
        print(f"   - {structured_ru}")
        print(f"   - {guide_ru}")
        print(f"   - {professor_ru}")
        print(f"\nâœ… English summaries saved:")
        print(f"   - {structured_en}")
        print(f"   - {guide_en}")
        print(f"   - {professor_en}")
        
        return {
            'russian': {
                'structured_path': str(structured_ru),
                'guide_path': str(guide_ru),
                'professor_path': str(professor_ru)
            },
            'english': {
                'structured_path': str(structured_en),
                'guide_path': str(guide_en),
                'professor_path': str(professor_en),
                'translated_transcript_path': str(translated_path)
            }
        }

    def _translate_to_english(self, russian_text: str) -> str:
        """Translate Russian transcript to English."""
        # Split into chunks if text is too long
        chunks = chunk_text(russian_text, self.chunk_chars * 2, 0)  # Larger chunks for translation
        
        translated_chunks = []
        for idx, chunk in enumerate(chunks, 1):
            print(f"   Translating chunk {idx}/{len(chunks)}...")
            prompt = TRANSLATION_PROMPT.format(russian_text=chunk)
            translation = self._generate_text(prompt, self.russian_model)
            translated_chunks.append(translation.strip())
        
        return "\n\n".join(translated_chunks)

    def _generate_language_summaries(self, raw_text, cleaned_text, language, prompts, model):
        """Generate structured, guide, and professor summaries for a specific language."""
        # Generate structured chunks
        structured_chunks, raw_chunks, cleaned_chunks = self._generate_structured_chunks(
            raw_text, cleaned_text, prompts['narrative'], model
        )
        structured_body = "\n\n".join(chunk for chunk in structured_chunks if chunk).strip()
        
        # Generate guide
        guide_body = self._generate_guide(
            structured_chunks, cleaned_chunks, raw_chunks, prompts['guide'], model
        )
        
        # Generate professor recap
        professor_body = self._generate_professor(
            structured_chunks, cleaned_chunks, raw_chunks, prompts['professor'], model
        )
        
        return {
            'structured': structured_body,
            'guide': guide_body,
            'professor': professor_body
        }

    def _generate_structured_chunks(self, raw_text, cleaned_text, prompt_template, model):
        """Generate structured narrative chunks."""
        raw_chunks = chunk_text(raw_text, self.chunk_chars, self.chunk_overlap)
        
        print(f"ðŸ” Created {len(raw_chunks)} chunks from {len(raw_text):,} characters")
        for idx, chunk in enumerate(raw_chunks, 1):
            print(f"   Chunk {idx}: {len(chunk):,} chars")
        
        cleaned_chunks = []
        structured_chunks = []
        recent_headings = []
        
        total = len(raw_chunks)
        if total == 0:
            return structured_chunks, raw_chunks, cleaned_chunks
        
        for idx, raw_chunk in enumerate(raw_chunks, start=1):
            print(f"ðŸ”„ Processing structured chunk {idx}/{total}...")
            
            cleaned_chunk = clean_transcript_text(raw_chunk)
            cleaned_chunks.append(cleaned_chunk)
            
            prior_topics = "\n".join(recent_headings[-5:]) if recent_headings else "None yet."
            context_instruction = (
                "First chunk: start with `# Classroom Lesson Narrative` and a `> [!note] Context` callout summarizing the lecture goals before other sections."
                if idx == 1 else
                "Continue with new `##` sections only; do NOT repeat the top-level header or context callout."
            )
            
            prompt = prompt_template.format(
                chunk_index=idx,
                total_chunks=total,
                prior_topics=prior_topics,
                context_instruction=context_instruction,
                cleaned_chunk=cleaned_chunk,
                raw_chunk=raw_chunk
            )
            
            response = self._generate_text(prompt, model)
            structured_chunk = response.strip()
            structured_chunks.append(structured_chunk)
            recent_headings.extend(self._extract_headings(structured_chunk))
        
        return structured_chunks, raw_chunks, cleaned_chunks

    def _generate_guide(self, structured_chunks, cleaned_chunks, raw_chunks, prompt_template, model):
        """Generate action guide."""
        aggregate = OrderedDict(
            mission_control=[],
            key_concepts=[],
            assignments=[],
            study_theory=[],
            study_practice=[],
            study_admin=[],
            exam_intel=[],
            risk_followups=[],
            next_moves=[],
        )
        
        total = len(structured_chunks)
        for idx, (structured_chunk, cleaned_chunk, raw_chunk) in enumerate(
            zip(structured_chunks, cleaned_chunks, raw_chunks), start=1
        ):
            print(f"ðŸ”„ Processing guide chunk {idx}/{total}...")
            
            prompt = prompt_template.format(
                chunk_index=idx,
                total_chunks=total,
                structured_chunk=structured_chunk,
                cleaned_chunk=cleaned_chunk,
                raw_chunk=raw_chunk
            )
            
            response = self._generate_text(prompt, model)
            chunk_data = self._parse_guide_json(response)
            for key in aggregate:
                self._extend_unique(aggregate[key], chunk_data.get(key, []))
        
        if not aggregate['assignments']:
            aggregate['assignments'].append('- [ ] Confirm: no assignments announced this session.')
        if not aggregate['mission_control']:
            aggregate['mission_control'].append('No additional summary beyond structured notes.')
        
        aggregate['next_moves'] = aggregate['next_moves'][:3]
        
        lines = []
        lines.append('## Mission Control')
        lines.append("\n".join(aggregate['mission_control']).strip())
        
        lines.append('\n## Key Concepts & Definitions')
        lines.extend(self._ensure_bullets(aggregate['key_concepts']))
        
        lines.append('\n## Assignments, Projects, Exams')
        lines.extend(self._ensure_checkboxes(aggregate['assignments']))
        
        lines.append('\n## Study & Revision Checklist')
        lines.append('### Theory')
        lines.extend(self._ensure_checkboxes(aggregate['study_theory']))
        lines.append('\n### Practice')
        lines.extend(self._ensure_checkboxes(aggregate['study_practice']))
        lines.append('\n### Admin')
        lines.extend(self._ensure_checkboxes(aggregate['study_admin']))
        
        lines.append('\n## Exam Intel')
        lines.extend(self._ensure_bullets(aggregate['exam_intel']))
        
        lines.append('\n## Risk & Follow-ups')
        lines.extend(self._ensure_bullets(aggregate['risk_followups']))
        
        lines.append('\n## Next Moves')
        lines.extend(self._ensure_bullets(aggregate['next_moves']))
        
        return "\n".join(line for line in lines if line is not None)

    def _generate_professor(self, structured_chunks, cleaned_chunks, raw_chunks, prompt_template, model):
        """Generate professor recap."""
        outputs = []
        recent_headings = []
        total = len(structured_chunks)
        
        for idx, (structured_chunk, cleaned_chunk, raw_chunk) in enumerate(
            zip(structured_chunks, cleaned_chunks, raw_chunks), start=1
        ):
            print(f"ðŸ”„ Processing professor chunk {idx}/{total}...")
            
            prior_topics = "; ".join(recent_headings[-5:]) if recent_headings else "None yet."
            prompt = prompt_template.format(
                chunk_index=idx,
                total_chunks=total,
                prior_topics=prior_topics,
                structured_chunk=structured_chunk,
                cleaned_chunk=cleaned_chunk,
                raw_chunk=raw_chunk
            )
            
            response = self._generate_text(prompt, model)
            recap = response.strip()
            outputs.append(recap)
            recent_headings.extend(self._extract_headings(recap))
        
        return "\n\n".join(output for output in outputs if output)

    def _generate_text(self, prompt: str, model: str) -> str:
        """Generate text using specified model."""
        response = self.client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=self.max_tokens,
            temperature=self.temperature
        )
        return response.choices[0].message.content

    def _compose_document(self, frontmatter: str, body: str) -> str:
        """Compose final document with frontmatter."""
        body = body.strip()
        if not body:
            body = "*(No content generated.)*"
        return f"{frontmatter}\n\n{body}\n"

    def _extract_headings(self, markdown: str):
        """Extract markdown headings."""
        headings = []
        if not markdown:
            return headings
        for line in markdown.splitlines():
            clean = line.strip()
            if clean.startswith('##'):
                headings.append(clean.lstrip('#').strip())
        return headings

    def _parse_guide_json(self, response: str):
        """Parse JSON response from guide generation."""
        if not response:
            return {}
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            try:
                start = response.index('{')
                end = response.rindex('}') + 1
                return json.loads(response[start:end])
            except (ValueError, json.JSONDecodeError):
                return {}

    def _extend_unique(self, target_list, items):
        """Extend list with unique items."""
        if not items:
            return
        for item in items:
            cleaned = item.strip()
            if cleaned and cleaned not in target_list:
                target_list.append(cleaned)

    def _ensure_bullets(self, items):
        """Ensure items are formatted as bullets."""
        if not items:
            return ['- None recorded.']
        bullets = []
        for item in items:
            line = item.strip()
            if not line:
                continue
            if not line.startswith('-') and not line.startswith('>'):
                line = f"- {line}"
            bullets.append(line)
        return bullets or ['- None recorded.']

    def _ensure_checkboxes(self, items):
        """Ensure items are formatted as checkboxes."""
        if not items:
            return ['- [ ] None recorded.']
        boxes = []
        for item in items:
            line = item.strip()
            if not line:
                continue
            if not line.startswith('- ['):
                line = f"- [ ] {line.lstrip('-').strip()}"
            boxes.append(line)
        return boxes or ['- [ ] None recorded.']